{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Linear and Logistic Regression\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this exercise, you will implement linear regression and logistic regression variants and see how it works on some simple datasets. Before the tasks most important equations are explained again. You might wanna use the lecture slides / notes as well.\n",
    "\n",
    "As in assignment 1 please submit your solutions with the grading system (you need to be either inside the HM network or connected to eduVPN to reach the online submission service). Additionally please upload a PDF of the notebook to the Moddle course with all result cells visible (see last section of this notebook). \n",
    "\n",
    "The previous environment (python venv) used for assignment 1 should also work here to execute everything in this second assignment. See assignemnt 1 folder for the requirements.txt which can be installed with \"pip install -r exercise-requirements.txt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import utils\n",
    "\n",
    "grader=utils.Grader()\n",
    "\n",
    "# SET YOUR Authentication Token. To get the token login to http://evalml.da.private.hm.edu/ and check the User info menu item.\n",
    "AUTH_TOKEN = \"cd4a9aa7782d5a61a41c6dd48746e426d1f9a09a\"\n",
    "grader.setToken(AUTH_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission and Grading\n",
    "\n",
    "After completing each part of the assignment you are asked to submit your results\n",
    "\n",
    "In this second assignment we will cover linear regression with one feature dimension and logistic regression with two feature dimensions, as well as cost functions and parameter learning.\n",
    "\n",
    "**Required Exercises**\n",
    "\n",
    "\n",
    "| Task    | Part                                           |Submitted Function                     | Points \n",
    "|---------|:-                                             |:-                                     | :-:     \n",
    "| 1       | [Compute cost for linear regression](#computeCost)     | [`computeCost`](#computeCost) | 20      |\n",
    "| 2       | [Closed form solution linear regression](#normalEqn)                        | [`normalEqn`](#normalEqn)        | 15      |\n",
    "| 3       | [Gradient descent linear regression](#gradientDescent) | [`gradientDescent`](#gradientDescent) | 20\n",
    "| 4       | [Sigmoid Function](#sigmoid)                   | [`sigmoid`](#sigmoid) | 5      \n",
    "| 5       | [Compute cost for logistic regression](#costFunctionLog) | [`costFunctionLog`](#costFunctionLog) | 20     \n",
    "| 6       | [Gradient descent logistic regression](#gradientDescentLog)     | [`gradientDescentLog`](#gradientDescentLog) | 15     \n",
    "| 7       | [Predict Function logistic regression](#predict)                     | [`predict`](#predict) | 5         | \n",
    "|         | Total Points                                   |                                       | 100    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You are allowed to submit your solutions multiple times. Correct results are not updated anymore.\n",
    "\n",
    "At the end of each section, we have a cell which contains code for submitting the solutions thus far to the grader. Execute the cell to see your score up to the current section. For all your work to be submitted properly, you must execute those cells at least once. They must also be re-executed everytime the submitted function is updated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Linear regression\n",
    "\n",
    "In this first chapter of the assignment you will implement linear regression using a one-dimensional input and a one-dimensional target variable. \n",
    "\n",
    "The file `Data/studenthours.txt` contains the dataset for our linear regression problem. The dataset contains 100 2d data points. This toy dataset provides a relation between study hours on the machine learning topic and gained course grades (Note: fictive dataset!).\n",
    "The first column are the student hours and the second column is the grade gained at the end. \n",
    "\n",
    "The dataset is loaded from the data file into the variables `X` and `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read comma separated data\n",
    "dataLinReg = np.loadtxt(os.path.join('Data', 'studenthours.txt'), delimiter=',')\n",
    "XLin, yLin = dataLinReg[:, 0], dataLinReg[:, 1]\n",
    "\n",
    "NLin = yLin.size  # number of training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of linear regression is to minimize the cost function\n",
    "\n",
    "$$ C(w) = \\sum_{i=1}^N \\left( h_{w}(x_{i}) - y_{i}\\right)^2$$\n",
    "\n",
    "where the hypothesis $h_w(x)$ is given by the linear model\n",
    "$$ h_w(x) = w^Tx = w_0 + w_1 x_1$$\n",
    "\n",
    "Recall that the parameters of your model are the $w_k$ values. These are\n",
    "the values you will adjust to minimize cost $C(w)$. One way to do this is to\n",
    "use the batch gradient descent algorithm. In batch gradient descent, each\n",
    "iteration performs the update\n",
    "\n",
    "$$ w_k = w_k - \\alpha \\sum_{i=1}^N \\left( h_w(x_{i}) - y_{i}\\right)x_{ik} \\qquad \\text{simultaneously update } w_k \\text{ for all } k$$\n",
    "\n",
    "With each step of gradient descent, your parameters $w_k$ come closer to the optimal values that will achieve the lowest cost C($w$).\n",
    "\n",
    "Note on implementation: We store each example as a row in the the $X$ matrix (i.e. the Design matrix) in a Python `numpy` array. To take into account the intercept term ($w_0$), we add an additional first column to $X$ and set it to all ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Plotting the Data\n",
    "\n",
    "Before starting on any task, it is often a good idea to understand the data by visualizing it. For this dataset, you can use a scatter plot to visualize the data, since it has only two properties to plot (student hours and grades). Many other problems that you will encounter in real life are multi-dimensional and cannot be plotted on a 2-d plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(x, y):\n",
    "    \"\"\"\n",
    "    Plots the data points x and y into a new figure. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like\n",
    "        Data point values for x-axis.\n",
    "\n",
    "    y : array_like\n",
    "        Data point values for y-axis. Note x and y should have the same size.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Plot the training data into a figure using the \"figure\" and \"plot\"\n",
    "    functions. Set the axes labels using the \"xlabel\" and \"ylabel\" functions.\n",
    "    Assume the study hours and grades have been passed in as the x\n",
    "    and y arguments of this function.    \n",
    "    \n",
    "    Hint\n",
    "    ----\n",
    "    You can use the 'ro' option with plot to have the markers\n",
    "    appear as red circles. Furthermore, you can make the markers larger by\n",
    "    using plot(..., 'ro', ms=10), where `ms` refers to marker size. You \n",
    "    can also set the marker edge color using the `mec` property.\n",
    "    \"\"\"\n",
    "    fig = pyplot.figure()  # open a new figure\n",
    "    \n",
    "    pyplot.plot(x, y, 'ro', ms=10, mec='k')\n",
    "    pyplot.xlim([20,80])\n",
    "    pyplot.ylim([30,120])\n",
    "    pyplot.ylabel('Grade')\n",
    "    pyplot.xlabel('Study hours')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotData(XLin, yLin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Implementation\n",
    "\n",
    "#### 1.2.1 Adding the intercept term\n",
    "\n",
    "We have already set up the data for linear regression. In the following cell, we add another dimension to our data to accommodate the $w_0$ intercept term. Do NOT execute this cell more than once (otherwise you wil add more and more ones to the top of the vector...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column of ones to X. The numpy function stack joins arrays along a given axis. \n",
    "# The first axis (axis=0) refers to rows (training examples) \n",
    "# and second axis (axis=1) refers to columns (features).\n",
    "XLin = np.stack([np.ones(NLin), XLin], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "#### 1.2.1 Computing the cost $C(w)$\n",
    "\n",
    "As you perform gradient descent to learn minimize the cost function $C(w)$, it is helpful to monitor the convergence by computing the cost. In this section, you will implement a function to calculate $C(w)$ so you can check the convergence of your gradient descent implementation. \n",
    "\n",
    "Your next task is to complete the code for the function `computeCost` which computes $C(w)$. As you are doing this, remember that the variables $X$ and $y$ are not scalar values. $X$ is a matrix whose rows represent the examples from the training set and $y$ is a vector whose each element represent the value at a given row of $X$.\n",
    "<a id=\"computeCost\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(X, y, w):\n",
    "    \"\"\"\n",
    "    Compute cost for linear regression. Computes the cost of using w as the\n",
    "    parameter for linear regression to fit the data points in X and y.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The input dataset of shape (N x M+1), where N is the number of examples,\n",
    "        and M is the number of features. We assume a vector of one's already \n",
    "        appended to the features so we have M+1 columns.\n",
    "    \n",
    "    y : array_like\n",
    "        The values of the function at each data point. This is a vector of\n",
    "        shape (N, ).\n",
    "    \n",
    "    w : array_like\n",
    "        The parameters for the regression function. This is a vector of \n",
    "        shape (M+1, ).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    C : float\n",
    "        The value of the regression cost function.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the cost of a particular choice of w. \n",
    "    You should set J to the cost.\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize some useful values\n",
    "    N = y.size  # number of training examples\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    C = 0.0\n",
    "\n",
    "    # ====================== YOUR CODE HERE =====================\n",
    "    C = np.sum((np.dot(X, w) - y)**2)\n",
    "\n",
    "    \n",
    "    # ===========================================================\n",
    "\n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have completed the function, the next step will run `computeCost` two times using two different initializations of $w$. You will see the cost printed to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = computeCost(XLin, yLin, w=np.array([0.0, 0.0]))\n",
    "print('With w = [0, 0] \\nCost computed = %.2f' % C)\n",
    "print('Expected cost value (approximately) 603923.07\\n')\n",
    "\n",
    "# further testing of the cost function\n",
    "C = computeCost(XLin, yLin, w=np.array([-1, 2]))\n",
    "print('With w = [-1, 2]\\nCost computed = %.2f' % C)\n",
    "print('Expected cost value (approximately) 73516.93')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You should now submit your solutions by executing the following cell.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appends the implemented function to the grader object\n",
    "grader.setFunc(\"computeCost\", computeCost)\n",
    "newfunc = grader.grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section7\"></a>\n",
    "#### 1.2.2 Closed-form solution\n",
    "\n",
    "Remember the closed-form, analytical solution to linear regression (see lecture slides for derivation):\n",
    "\n",
    "$$ \\bm{w} = \\left( \\bm{X}^T \\bm{X}\\right)^{-1} \\bm{X}^T\\bm{y}$$\n",
    "\n",
    "This gives you directly an exact solution without the need using an iterative method like gradient descent.\n",
    "\n",
    "Let's reload the data and add the intercept term again to be included implicitely in the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = np.loadtxt(os.path.join('Data', 'studenthours.txt'), delimiter=',')\n",
    "XLin, yLin = data[:, 0], data[:, 1]\n",
    "NLin = yLin.size\n",
    "XLin = np.stack([np.ones(NLin), XLin], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the code for the function `normalEqn` below to use the formula above to calculate $w$. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Note the we can use matrix multiplication, rather than explicit summation or looping. This is also called code vectorization.\n",
    "</div>\n",
    "\n",
    "<a id=\"normalEqn\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalEqn(X, y):\n",
    "    \"\"\"\n",
    "    Computes the closed-form solution to linear regression using the normal equations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The dataset of shape (N x M+1).\n",
    "    \n",
    "    y : array_like\n",
    "        The value at each data point. A vector of shape (N, ).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    w : array_like\n",
    "        Estimated linear regression parameters. A vector of shape (M+1, ).\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Complete the code to compute the closed form solution to linear\n",
    "    regression and put the result in w.\n",
    "    \n",
    "    \"\"\"\n",
    "    w = np.zeros(X.shape[1])\n",
    "    \n",
    "    # ===================== YOUR CODE HERE ============================\n",
    "    # part 1 of the derivation of the normal equation (X^T*X)^-1\n",
    "    p1 = np.linalg.inv(np.dot(np.transpose(X), X))\n",
    "    # part 2 of the derivation of the normal equation X^T*y\n",
    "    p2 = np.dot(np.transpose(X), y)\n",
    "    w = (np.dot(p1, p2))\n",
    "    \n",
    "    # =================================================================\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can plot the found linear model. This should look similar to this one:<br>\n",
    "![](Figures/studyhours_plot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the linear fit\n",
    "plotData(XLin[:, 1], yLin)\n",
    "w = normalEqn(XLin, yLin)\n",
    "pyplot.plot(XLin[:, 1], np.dot(XLin, w), '-')\n",
    "pyplot.ylabel('Grade')\n",
    "pyplot.xlabel('Study hours')\n",
    "pyplot.xlim([20,80])\n",
    "pyplot.ylim([30,120])\n",
    "pyplot.legend(['Training data', 'Linear regression']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two more checks for you: 1) Compare the weight parameters. 2) Given the model parameters $w$ you would like to predict the grade based on an input of 60 study hours. This should result in grade of 88."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the parameters from the normal equation\n",
    "w_neq = normalEqn(XLin, yLin)\n",
    "\n",
    "# Display normal equation's result\n",
    "print('w computed from the normal equations: {:s} (should be [21.95116112  1.09652181])'.format(str(w_neq)))\n",
    "\n",
    "# Estimate the Grade when investing 60 Study Hours\n",
    "# ====================== YOUR CODE HERE ======================\n",
    "\n",
    "\n",
    "# You should change this and add the correc term for predicting a grade based on 60 study hours\n",
    "hours = 0 \n",
    "\n",
    "# ============================================================\n",
    "\n",
    "print('Predicted Grade when investing 60 Study Hours (using normal equations): {:.0f} (should be 88)'.format(hours))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds the implemented function to the grader object\n",
    "grader.setFunc(\"normalEqn\", normalEqn)\n",
    "grader.grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "#### 1.2.3 Gradient Descent\n",
    "\n",
    "In this part, you will fit the linear regression parameters $w$ to our dataset using gradient descent.\n",
    "\n",
    "The objective of linear regression is to minimize the cost function (note constant $1/N$ already removed)\n",
    "\n",
    "$$ C(w) = \\sum_{i=1}^N \\left( h_{w}(x_{i}) - y_{i}\\right)^2$$\n",
    "\n",
    "where the hypothesis $h_w(x)$ is given by the linear model\n",
    "$$ h_w(x) = w^Tx = w_0 + w_1 x_1$$\n",
    "\n",
    "Recall that the parameters of your model are the $w_j$ values. These are\n",
    "the values you will adjust to minimize cost $C(w)$. One way to do this is to\n",
    "use the batch gradient descent algorithm. In batch gradient descent, each\n",
    "iteration performs the update\n",
    "\n",
    "$$ w_k = w_k - \\alpha \\sum_{i=1}^N \\left( h_w(x_i) - y_{i}\\right)x_{ik} \\qquad \\text{simultaneously update } w_k \\text{ for all } k$$\n",
    "\n",
    "With each step of gradient descent, your parameters $w_j$ come closer to the optimal values that will achieve the lowest cost C($w$).\n",
    "\n",
    "**Implementation Note:** We store each example as a row in the the $X$ matrix in Python `numpy`. To take into account the intercept term ($w_0$), we add an additional first column to $X$ and set it to all ones. This allows us to treat $w_0$ as simply another 'feature'.\n",
    "\n",
    "Next, you will complete a function which implements gradient descent.\n",
    "The loop structure has been written for you, and you only need to supply the updates to $w$ within each iteration. \n",
    "\n",
    "As you program, make sure you understand what you are trying to optimize and what is being updated. Keep in mind that the cost $C(w)$ is parameterized by the vector $w$, not $X$ and $y$. That is, we minimize the value of $C(w)$ by changing the values of the vector $w$, not by changing $X$ or $y$. Refer to lecture slides.\n",
    "\n",
    "The starter code for the function `gradientDescent` calls `computeCost` on every iteration and saves the cost to a `python` list. Assuming you have implemented gradient descent and `computeCost` correctly, your value of $C(w)$ should never increase, and should converge to a steady value by the end of the algorithm. Maybe you could add another cell to check this behavior in a plot, where you ploting the loss over performed iterations (e.g. using matplotlib).\n",
    "\n",
    "<a id=\"gradientDescent\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X, y, w, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to learn `w`. Updates w by taking `num_iters`\n",
    "    gradient steps with learning rate `alpha`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The input dataset of shape (N x M+1).\n",
    "    \n",
    "    y : array_like\n",
    "        Value at given features. A vector of shape (N, ).\n",
    "    \n",
    "    w : array_like\n",
    "        Initial values for the linear regression parameters. \n",
    "        A vector of shape (M+1, ).\n",
    "    \n",
    "    alpha : float\n",
    "        The learning rate.\n",
    "    \n",
    "    num_iters : int\n",
    "        The number of iterations for gradient descent. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    w : array_like\n",
    "        The learned linear regression parameters. A vector of shape (M+1, ).\n",
    "    \n",
    "    C_history : list\n",
    "        A python list for the values of the cost function after each iteration.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Peform a single gradient step on the parameter vector w.\n",
    "\n",
    "    While debugging, it can be useful to print out the values of \n",
    "    the cost function (computeCost) and gradient here.\n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    N = y.shape[0]  # number of training examples\n",
    "    \n",
    "    # make a copy of w, to avoid changing the original array, since numpy arrays\n",
    "    # are passed by reference to functions\n",
    "    w = w.copy()\n",
    "    \n",
    "    C_history = [] # Use a python list to save cost in every iteration\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # ==================== YOUR CODE HERE =================================\n",
    "        w[i] = w[i] - alpha * np.sum((np.dot(X,w) - y) * X[:i])\n",
    "\n",
    "        # =====================================================================\n",
    "        \n",
    "        # save the cost J in every iteration\n",
    "        C_history.append(computeCost(X, y, w))\n",
    "    \n",
    "    return w, C_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you are finished call the implemented `gradientDescent` function and print the computed $w$. We initialize the $w$ parameters to 0 and the learning rate $\\alpha$ to 0.000001. Execute the following cell to check your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = np.loadtxt(os.path.join('Data', 'studenthours.txt'), delimiter=',')\n",
    "XLin, yLin = data[:, 0], data[:, 1]\n",
    "NLin = yLin.size\n",
    "XLin = np.stack([np.ones(NLin), XLin], axis=1)\n",
    "\n",
    "# initialize fitting parameters\n",
    "w = np.zeros(2)\n",
    "\n",
    "# some gradient descent settings\n",
    "iterations = 1000\n",
    "\n",
    "# Note: The learnind rate should be quite small since we ignored the constant 1/N in the cost function (i.e. dividing the cost function by the number of samples.)\n",
    "alpha = 0.000001 \n",
    "\n",
    "w, C_history = gradientDescent(XLin ,yLin, w, alpha, iterations)\n",
    "print('Computed cost is {}'.format(computeCost(XLin, yLin, w)))\n",
    "print('w found by gradient descent: [{:.4f}, {:.4f}]'.format(*w))\n",
    "print('Expected w values (approximately): [0.1126, 1.5258]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use your final parameters to plot the linear fit. The results should look like the following figure.\n",
    "\n",
    "![](Figures/studyhours_gd_plot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the linear fit\n",
    "plotData(XLin[:, 1], yLin)\n",
    "pyplot.plot(XLin[:, 1], np.dot(XLin, w), '-')\n",
    "pyplot.xlim([20,80])\n",
    "pyplot.ylim([30,120])\n",
    "pyplot.legend(['Training data', 'Linear regression']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your final values for $w$ will also be used to make predictions on grades for 40 and 60 study hours spent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict values for population sizes of 40 and 60 study hours\n",
    "predict1 = np.dot([1, 40], w)\n",
    "print('For 40 study hours, we predict a grade of {:.2f} (expected: ~61.14) \\n'.format(predict1))\n",
    "\n",
    "predict2 = np.dot([1, 60], w)\n",
    "print('For 60 study hours, we predict a grade of {:.2f} (expected: ~91.66)\\n'.format(predict2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You should now submit your solutions by executing the next cell.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appends the implemented function to the grader object\n",
    "grader.setFunc(\"gradientDescent\", gradientDescent)\n",
    "grader.grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Visualizing $C(w)$\n",
    "\n",
    "To understand the cost function $C(w)$ better, you will now plot the cost over a 2-dimensional grid of $w_0$ and $w_1$ values. \n",
    "\n",
    "In the next cell, the code is already written to compute $C(w)$ over a grid of values using the `computeCost` function. After executing the following cell, you will have a 2-D array of $C(w)$ values. Then, those values are used to produce surface and contour plots of $C(w)$ using the matplotlib `plot_surface` and `contourf` functions. The plots should look something like the following:\n",
    "\n",
    "![](Figures/contour_plot.png)\n",
    "\n",
    "The purpose of these graphs is to show you how $C(w)$ varies with changes in $w_0$ and $w_1$. The cost function $C(w)$ is bowl-shaped and has a global minimum (although the cost function became really flat around this point). This optimal minimum (global minimum, geen point) has been computed using the closed-form solution. Each step of gradient descent moves closer to this this optimal minimum, but since the gradient gets very small at the end it is hard to reach to optimal minimum using gradient descent (red point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid over which we will calculate J\n",
    "w0_vals = np.linspace(-40, 40, 100)\n",
    "w1_vals = np.linspace(0, 3, 100)\n",
    "\n",
    "# initialize J_vals to a matrix of 0's\n",
    "J_vals = np.zeros((w0_vals.shape[0], w1_vals.shape[0]))\n",
    "\n",
    "# Fill out J_vals\n",
    "for i, w0 in enumerate(w0_vals):\n",
    "    for j, w1 in enumerate(w1_vals):\n",
    "        J_vals[i, j] = computeCost(XLin, yLin, [w0, w1])\n",
    "        \n",
    "# Because of the way meshgrids work in the surf command, we need to\n",
    "# transpose J_vals before calling surf, or else the axes will be flipped\n",
    "J_vals = J_vals.T\n",
    "\n",
    "# surface plot\n",
    "fig = pyplot.figure(figsize=(12, 5))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.plot_surface(w0_vals, w1_vals, J_vals, cmap='viridis')\n",
    "pyplot.xlabel('w0')\n",
    "pyplot.ylabel('w1')\n",
    "pyplot.title('Surface')\n",
    "\n",
    "# contour plot\n",
    "# Plot J_vals as 15 contours spaced logarithmically between 0.01 and 100\n",
    "ax = pyplot.subplot(122)\n",
    "pyplot.contour(w0_vals, w1_vals, J_vals, linewidths=2, cmap='viridis', levels=100)#levels=np.logspace(-2, 3, 20))\n",
    "pyplot.xlabel('w0')\n",
    "pyplot.ylabel('w1')\n",
    "pyplot.plot(w[0], w[1], 'ro', ms=10, lw=2)\n",
    "pyplot.plot(w_neq[0], w_neq[1], 'go', ms=10, lw=2)\n",
    "pyplot.title('Contour, showing minimum')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You should now submit your solutions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Logistic Regression\n",
    "\n",
    "In this part of the exercise, you will build a logistic regression model to predict whether \n",
    "Suppose you are the product manager of a microchip factory and you have meassured for some microchips two different features. From these two features, you would like to determine whether the microchips should be accepted or rejected. To help you make the decision, you have a dataset of feature meassures on microchips produced in the past, from which you can build a logistic regression model. \n",
    "\n",
    "The following cell will load the data and corresponding labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import mapFeature\n",
    "\n",
    "# Load Data\n",
    "# note this data is already normalized\n",
    "data = np.loadtxt(os.path.join('Data', 'microchips.txt'), delimiter=',')\n",
    "\n",
    "# The first two columns contains the X values and the third column\n",
    "# contains the label (y).\n",
    "\n",
    "sel = np.ones(data.shape[0])\n",
    "\n",
    "# You might wanna select only part of data\n",
    "# sel = data[:, 0]<0.5\n",
    "\n",
    "XLog = data[sel==1, :2]\n",
    "yLog = data[sel==1, 2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Visualizing the data\n",
    "\n",
    "Before starting to implement the learning algorithm, it is again good to visualize the data if possible. We display the data on a 2-dimensional plot by calling the function `plotData`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(X, y):\n",
    "    \"\"\"\n",
    "    Plots the data points X and y into a new figure. Plots the data \n",
    "    points with * for the positive examples and o for the negative examples.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        An Nx2 matrix representing the dataset. \n",
    "    \n",
    "    y : array_like\n",
    "        Label values for the dataset. A vector of size (N, ).\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Plot the positive and negative examples on a 2D plot, using the\n",
    "    option 'k*' for the positive examples and 'ko' for the negative examples.    \n",
    "    \"\"\"\n",
    "    # Create New Figure\n",
    "    fig = pyplot.figure()\n",
    "\n",
    "    pos = y == 1\n",
    "    neg = y == 0\n",
    "\n",
    "    # Plot Examples\n",
    "    pyplot.plot(X[pos, 0], X[pos, 1], 'k*',  mfc='y', lw=2, ms=10)\n",
    "    pyplot.plot(X[neg, 0], X[neg, 1], 'ko', mfc='b', ms=8, mec='k', mew=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we call the implemented function to display the loaded data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotData(XLog, yLog)\n",
    "# Labels and Legend\n",
    "pyplot.xlabel('Microchip Feature 1')\n",
    "pyplot.ylabel('Microchip Feature 2')\n",
    "\n",
    "# Specified in plot order\n",
    "pyplot.legend(['y = 1', 'y = 0'], loc='upper right')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Implementation\n",
    "\n",
    "#### 2.2.1 The sigmoid function\n",
    "\n",
    "Before you start with the actual cost function, recall that the logistic regression hypothesis is defined as:\n",
    "\n",
    "$$ h_w(x) = g(w^T x)$$\n",
    "\n",
    "where function $g$ is the sigmoid function. The sigmoid function is defined as: \n",
    "\n",
    "$$ g(z) = \\frac{1}{1+e^{-z}} $$\n",
    "\n",
    "Your first step is to implement this function `sigmoid` so it can be called by the following functions. When you are finished, try testing a few\n",
    "values by calling `sigmoid(x)` in a new cell. For large positive values of `x`, the sigmoid should be close to 1, while for large negative values, the sigmoid should be close to 0. Evaluating `sigmoid(0)` should give you exactly 0.5. \n",
    "Your code should also work with vectors and matrices. For a matrix, your function should perform the sigmoid function on every element.\n",
    "<a id=\"sigmoid\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute sigmoid function given the input z.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : array_like\n",
    "        The input to the sigmoid function. This can be a 1-D vector \n",
    "        or a 2-D matrix. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    g : array_like\n",
    "        The computed sigmoid function. g has the same shape as z, since\n",
    "        the sigmoid is computed element-wise on z.\n",
    "        \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the sigmoid of each value of z (z can be a matrix, vector or scalar).\n",
    "    \"\"\"\n",
    "    # convert input to a numpy array\n",
    "    z = np.array(z)\n",
    "    \n",
    "    # You need to return the following variables correctly \n",
    "    g = np.zeros(z.shape)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    g = 1 / (1 + np.exp(-z))\n",
    "\n",
    "    # =============================================================\n",
    "    return np.float64(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell evaluates the sigmoid function at `z=0`. You should get a value of 0.5. You can also try different values for `z` to experiment with the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the implementation of sigmoid function here\n",
    "z = 0\n",
    "g = sigmoid(z)\n",
    "print('g(', z, ') = ', g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following cell to grade your solution to the first part of this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds the implemented function to the grader object\n",
    "grader.setFunc(\"sigmoid\", sigmoid)\n",
    "grader.grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Feature Transform\n",
    "\n",
    "As you noticed our dataset is highly non-linear. One way to tackle this is to use base functions to transform the features and to modify the feature space. In the function `mapFeature` defined in the file `utils.py`, we will map the features into all polynomial terms of $x_1$ and $x_2$ up to the eight power:\n",
    "\n",
    "$$ \\text{mapFeature}(x) = \\begin{bmatrix} 1 & x_1 & x_2 & x_1^2 & x_1 x_2 & x_2^2 & x_1^3 & \\dots & x_1 x_2^5 & x_2^6 \\end{bmatrix}^T $$\n",
    "\n",
    "Note: By this basis expension we can also experess dependencies between feature dimensions and do not work on each feature independentely.\n",
    "As a result of this mapping, our vector of two features (the scores on two QA tests) has been transformed into a 48-dimensional vector. A logistic regression classifier trained on this higher-dimension feature vector will have a more complex decision boundary and will appear nonlinear when drawn in our 2-dimensional plot.\n",
    "<br>Note: While the feature mapping allows us to build a more expressive classifier, it also more susceptible to overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XLog = utils.mapFeature(XLog[:, 0], XLog[:, 1])\n",
    "print(XLog.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Cost function and gradient\n",
    "\n",
    "Now you will implement code to compute the cost function and gradient for logistic regression. But this time (compared to above regression implementation) we would like to add a L2 regularization term.\n",
    "Complete the code for the function `costFunctionLog` below to return the cost and gradient.\n",
    "\n",
    "Recall that the Ridge Regression has the following form:\n",
    "\n",
    "$$ C(w) = \\sum_{i=1}^N \\left[ -y_{i}\\log \\left( h_w \\left(x_{i} \\right) \\right) - \\left( 1 - y_{i} \\right) \\log \\left( 1 - h_w \\left( x_{i} \\right) \\right) \\right] + \\lambda \\sum_{k=1}^M w_k^2 $$\n",
    "\n",
    "Note that the parameter $w_0$ should ot get reularizes, since this parameter has no effect on a specific feature dimension. The gradient of the cost function thus becomes::\n",
    "\n",
    "$$ \\frac{\\partial C(w)}{\\partial w_0} = \\sum_{i=1}^N \\left( h_w \\left(x_{i}\\right) - y_{i} \\right) x_{i} \\qquad \\text{for } k =0 $$\n",
    "\n",
    "$$ \\frac{\\partial C(w)}{\\partial w_j} = \\left( \\sum_{i=1}^N \\left( h_w \\left(x_{i}\\right) - y_{i} \\right) x_{i} \\right) + \\lambda w_k \\qquad \\text{for } k \\ge 1 $$\n",
    "<a id=\"costFunctionLog\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunctionLog(X, y,w, lambda_regularization=0):\n",
    "    \"\"\"\n",
    "    Compute cost and gradient for logistic regression with regularization.\n",
    "    \n",
    "    Parameters\n",
    "    w : array_like\n",
    "        Logistic regression parameters. A vector with shape (M, ). M is \n",
    "        the number of features including any intercept. If we have mapped\n",
    "        our initial features into polynomial features, then n is the total \n",
    "        number of polynomial features. \n",
    "    \n",
    "    X : array_like\n",
    "        The data set with shape (N x M). m is the number of examples, and\n",
    "        n is the number of features (i.e. after feature mapping if used).\n",
    "    \n",
    "    y : array_like\n",
    "        The data labels. A vector with shape (N, ).\n",
    "    \n",
    "    lambda_regularization : float\n",
    "        The regularization / weight decay parameter. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    C : float\n",
    "        The computed value for the regularized cost function. \n",
    "    \n",
    "    grad : array_like\n",
    "        A vector of shape (M, ) which is the gradient of the cost\n",
    "        function with respect to w, at the current values of w.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the cost `C` of a particular choice of w.\n",
    "    Compute the partial derivatives and set `grad` to the partial\n",
    "    derivatives of the cost w.r.t. each parameter in w.\n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    N = y.size  # number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    C = 0\n",
    "    grad = np.zeros(w.shape)\n",
    "\n",
    "    # ===================== YOUR CODE HERE ======================\n",
    "    h = sigmoid(np.dot(X, w))\n",
    "    C = np.sum(-y * np.log(h) - (1 - y) * np.log(1 - h)) + lambda_regularization/(2*N) * np.sum(w**2)\n",
    "    \n",
    "    # =============================================================\n",
    "    \n",
    "    return np.float64(C), np.float64(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use below tests to check if your cost and gradient gets correctly computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize fitting parameters\n",
    "initial_w = np.zeros(XLog.shape[1])\n",
    "\n",
    "# Set regularization parameter lambda to 1\n",
    "# DO NOT use `lambda` as a variable name in python\n",
    "# because it is a python keyword\n",
    "lambda_regularization = 1\n",
    "\n",
    "# Compute and display initial cost and gradient for regularized logistic\n",
    "# regression\n",
    "cost, grad = costFunctionLog(XLog, yLog, initial_w, lambda_regularization)\n",
    "print('Using a weight decay parameter of {:.8f}\\n'.format(lambda_regularization))\n",
    "print('Cost at initial w (zeros): {:.3f}'.format(cost))\n",
    "print('Expected cost (approx)       : 81.791\\n')\n",
    "\n",
    "print('Gradient at initial w (zeros) - first five values only:')\n",
    "print('\\t[{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}]'.format(*grad[:5]))\n",
    "print('Expected gradients (approx) - first five values only:')\n",
    "print('\\t[1.0000, 2.2170, 0.0092, 5.9407, 1.3572]\\n')\n",
    "\n",
    "\n",
    "# Compute and display cost and gradient\n",
    "# with all-ones w and lambda = 10\n",
    "lambda_regularization = 0.1\n",
    "test_w = np.ones(XLog.shape[1])\n",
    "cost, grad = costFunctionLog(XLog, yLog, test_w, lambda_regularization)\n",
    "\n",
    "print('------------------------------\\n')\n",
    "print('Using a weight decay parameter of {:.8f}\\n'.format(lambda_regularization))\n",
    "print('Cost at test w    : {:.2f}'.format(cost))\n",
    "print('Expected cost (approx): 282.29\\n')\n",
    "\n",
    "print('Gradient at initial w (zeros) - first five values only:')\n",
    "print('\\t[{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}]'.format(*grad[:5]))\n",
    "print('Expected gradients (approx) - first five values only:')\n",
    "print('\\t[40.9838, 9.2252, 13.1992, 16.9120, 0.9340]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You should now submit your solutions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appends the implemented function to the grader object\n",
    "grader.setFunc(\"costFunctionLog\", costFunctionLog)\n",
    "grader.grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 Gradient Descent\n",
    "\n",
    "In this part, you will fit the logistic regression parameters $w$ to our dataset using gradient descent. This time our function gets an additioanl attribute ($\\lambda$) to control the regularization force (weight decay).\n",
    "\n",
    "<a id=\"gradientDescentLog\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentLog(X, y, w, alpha, num_iters, lambda_regularization=1):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to learn `w`. Updates w by taking `num_iters`\n",
    "    gradient steps with learning rate `alpha`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The input dataset of shape (N x M+1).\n",
    "    \n",
    "    y : array_like\n",
    "        Value at given features. A vector of shape (N, ).\n",
    "    \n",
    "    w : array_like\n",
    "        Initial values for the linear regression parameters. \n",
    "        A vector of shape (M+1, ).\n",
    "    \n",
    "    alpha : float\n",
    "        The learning rate.\n",
    "    \n",
    "    num_iters : int\n",
    "        The number of iterations for gradient descent. \n",
    "\n",
    "    lambda_regularization : float\n",
    "        Weight decay parameter. Weight factor for regularization term.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    w : array_like\n",
    "        The learned linear regression parameters. A vector of shape (M+1, ).\n",
    "    \n",
    "    C_history : list\n",
    "        A python list for the values of the cost function after each iteration.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Peform a single gradient step on the parameter vector w.\n",
    "\n",
    "    While debugging, it can be useful to print out the values of \n",
    "    the cost function (computeCost) and gradient here.\n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    m = y.shape[0]  # number of training examples\n",
    "    \n",
    "    # make a copy of w, to avoid changing the original array, since numpy arrays\n",
    "    # are passed by reference to functions\n",
    "    w = w.copy()\n",
    "    \n",
    "    C_history = [] # Use a python list to save cost in every iteration\n",
    "    \n",
    "    cost = 0\n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # ==================== YOUR CODE HERE =================================\n",
    "        cost, grad = costFunctionLog(X, y, w, lambda_regularization)\n",
    "        # looop über welche werte?\n",
    "        w = w - alpha * grad\n",
    "\n",
    "\n",
    "        # =====================================================================\n",
    "        \n",
    "        # save the cost J in every iteration\n",
    "        C_history.append(cost)\n",
    "    \n",
    "    return np.float64(w), np.float64(C_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize fitting parameters\n",
    "w = np.zeros(XLog.shape[1])\n",
    "\n",
    "# some gradient descent settings\n",
    "iterations = 10000\n",
    "alpha = 0.1\n",
    "\n",
    "lambda_regularization = 0.01\n",
    "w, C_history = gradientDescentLog(XLog ,yLog, w, alpha, iterations, lambda_regularization)\n",
    "print('w found by gradient descent (first six): [{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}]'.format(*w))\n",
    "print('Expected w values (approximately): [3.7118, 1.8446, 4.7629, -5.7630, -6.6632, -5.2784]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now submit your solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appends the implemented function to the grader object\n",
    "grader.setFunc(\"gradientDescentLog\", gradientDescentLog)\n",
    "grader.grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "### 2.3 Evaluating logistic regression\n",
    "\n",
    "After having learned the paramater we would like to use them to predict the class of a new sample given its features. Your task is to complete the code in function `predict`. The predict function will produce “1” or “0” predictions given a dataset and a learned parameter vector $w$. \n",
    "<a id=\"predict\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, X):\n",
    "    \"\"\"\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression.\n",
    "    Computes the predictions for X using a threshold at 0.5 \n",
    "    (i.e., if sigmoid(w.T*x) >= 0.5, predict 1)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    w : array_like\n",
    "        Parameters for logistic regression. A vecotor of shape (M+1, ).\n",
    "    \n",
    "    X : array_like\n",
    "        The data to use for computing predictions. The rows is the number \n",
    "        of points to compute predictions, and columns is the number of\n",
    "        features.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    p : array_like\n",
    "        Predictions and 0 or 1 for each row in X. \n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Complete the following code to make predictions using your learned \n",
    "    logistic regression parameters.You should set p to a vector of 0's and 1's    \n",
    "    \"\"\"\n",
    "    m = X.shape[0] # Number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly\n",
    "    p = np.zeros(m)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    p = sigmoid(np.dot(X, w))\n",
    "    for i in range(len(p)):\n",
    "        if p[i] >= 0.5:\n",
    "            p[i] = 1\n",
    "        else:\n",
    "            p[i] = 0\n",
    "    p = p.astype(int)\n",
    "    # ============================================================\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have completed the code in `predict`, we proceed to report the training accuracy of your classifier by computing the percentage of examples classified correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Predict probability for a student with score 45 on exam 1 \n",
    "#  and score 85 on exam 2 \n",
    "# Compute accuracy on our training set\n",
    "p = predict(w, XLog)\n",
    "print('Train Accuracy: {:.2f} %'.format(np.mean(p == yLog) * 100))\n",
    "print('Expected accuracy (approx): 83.90 %')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now submit your solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appends the implemented function to the grader object\n",
    "grader.setFunc(\"predict\", predict)\n",
    "grader.grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the decision boundary of the unregularized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lean parameters (unregularized)\n",
    "w = np.zeros(XLog.shape[1])\n",
    "# some gradient descent settings\n",
    "iterations = 10000\n",
    "alpha = 0.1\n",
    "lambda_regularization = 0\n",
    "w, C_history = gradientDescentLog(XLog ,yLog, w, alpha, iterations, lambda_regularization)\n",
    "\n",
    "#Evaluate\n",
    "p = predict(w, XLog)\n",
    "print('Train Accuracy: {:.2f} %'.format(np.mean(p == yLog) * 100))\n",
    "\n",
    "# Plot Boundary\n",
    "utils.plotDecisionBoundary(plotData, w, XLog, yLog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the decision boundary of the unregularized version. Notice that the training accuracy goes slieghtly down, but the shape of the contour looks more reasonable. Especially for more complex models this regularization leads to a better generalization. In general the importance of the prior (i.e. the regularization) decreases when you have enough and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lean parameters (unregularized)\n",
    "w = np.zeros(XLog.shape[1])\n",
    "# some gradient descent settings\n",
    "iterations = 10000\n",
    "alpha = 0.1\n",
    "lambda_regularization = 0.01\n",
    "w, C_history = gradientDescentLog(XLog ,yLog, w, alpha, iterations,lambda_regularization)\n",
    "\n",
    "#Evaluate\n",
    "p = predict(w, XLog)\n",
    "print('Train Accuracy: {:.2f} %'.format(np.mean(p == yLog) * 100))\n",
    "\n",
    "# Plot Boundary\n",
    "utils.plotDecisionBoundary(plotData, w, XLog, yLog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Uploading your PDF\n",
    "Well done. You are now ready with this assignment. \n",
    "Please make sure that ALL code cells are executed and create a PDF from the notebook: Use the File-->Print... function in Juypter Lab (not via the Browser, and not export as PDF in Jupyter Lab).\n",
    "Upload this PDF to Moodle in the assignment section and in the correct assignment number.\n",
    "\n",
    "## 4 References\n",
    "Microchip Dataset (Logisitc Regression) from Andrew Ng, Machine Learning CS229/Coursera <br>\n",
    "Notebook has been inspired from Andrew Ng, Machine Learning CS229/Coursera"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlex_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
